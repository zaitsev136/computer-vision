{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, einsum\n",
    "from einops import rearrange\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim, axis=1, num_axes=4, eps=1e-5):\n",
    "        \"\"\"Layer normalization applied over axis 'axis' with the size 'dim'.\n",
    "        Total number of axes in the inputs and outputs are given by 'num_axes'\n",
    "\n",
    "        Args:\n",
    "            dim (int): size of the normalized dimension\n",
    "            axis (int, optional): Over which axis to normalize. Defaults to 1.\n",
    "            num_axes (int, optional): Number of axes. Defaults to 4.\n",
    "            eps (float, optional): epsilon. Defaults to 1e-5.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.axis = axis\n",
    "        shape = [1]*num_axes\n",
    "        shape[axis] = dim\n",
    "        self.w = nn.Parameter(torch.ones(shape))\n",
    "        self.b = nn.Parameter(torch.zeros(shape))\n",
    "\n",
    "    def forward(self, x):\n",
    "        std = torch.var(x, dim=self.axis, unbiased = False, keepdim = True).sqrt()\n",
    "        mean = torch.mean(x, dim=self.axis, keepdim = True)\n",
    "        return (x - mean) / (std + self.eps) * self.w + self.b\n",
    "    \n",
    "\n",
    "class SpatialReductionAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads, reduction):\n",
    "        \"\"\"Efficient spatial reduction multihead attention.\n",
    "\n",
    "        Args:\n",
    "            dim (int): dimensions\n",
    "            num_heads (int): number of heads\n",
    "            reduction (int): spatial reduction factor for K and V vectors\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.scale = 1.0/sqrt(dim // num_heads)\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.w_q = nn.Conv2d(dim, dim, 1, bias=False)\n",
    "        self.w_k = nn.Conv2d(dim, dim, reduction, stride=reduction, bias=False)\n",
    "        self.w_v = nn.Conv2d(dim, dim, reduction, stride=reduction, bias=False)\n",
    "        self.w_out = nn.Conv2d(dim, dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, w = x.size()[-2:]\n",
    "\n",
    "        q, k, v = (self.w_q(x), self.w_k(x), self.w_v(x))\n",
    "        multihead_rearrange = lambda t: rearrange(t, 'b (n d) h w -> (b n) d (h w)', n=self.num_heads)\n",
    "        q, k, v = map(multihead_rearrange, (q, k, v))\n",
    "\n",
    "        attn = einsum('b d i, b d j -> b i j', q, k) * self.scale\n",
    "        attn = attn.softmax(dim = -1)\n",
    "\n",
    "        out = einsum('b i j, b d j -> b d i', attn, v)\n",
    "        out = rearrange(out, '(b n) d (h w) -> b (n d) h w', n=self.num_heads, h=h)\n",
    "        return self.w_out(out)\n",
    "    \n",
    "\n",
    "class DepthwiseConv2d(nn.Module):\n",
    "    def __init__(self, c_in, c_out, kernel_size, stride=1, padding=0, bias=True):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(c_in, c_in, kernel_size=kernel_size, padding=padding,\n",
    "                      groups=c_in, stride=stride, bias=bias),\n",
    "            nn.Conv2d(c_in, c_out, kernel_size=1, bias=bias)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "\n",
    "class MixFNN(nn.Module):\n",
    "    def __init__(self, dim, expansion_factor):\n",
    "        \"\"\"Mix feed-forward network\n",
    "\n",
    "        Args:\n",
    "            dim (int): number of input and output dimensions\n",
    "            expansion_factor (int): ratio between hidden dimensions and input dimensions\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        hidden_dim = dim * expansion_factor\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Conv2d(dim, hidden_dim, 1),\n",
    "            DepthwiseConv2d(hidden_dim, hidden_dim, kernel_size=3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(hidden_dim, dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ff(x)\n",
    "\n",
    "\n",
    "class MiTStage(nn.Module):\n",
    "    def __init__(self, c_in, c_out, kernel, num_heads, attn_reduction, ff_expansion, num_layers):\n",
    "        \"\"\"MiT encoder building block. Normally, an MiT encoder consists of 4 such blocks, each generating\n",
    "        a feature map of different resolution.\n",
    "\n",
    "        Args:\n",
    "            c_in (int): number of input channels\n",
    "            c_out (int): number of output channels\n",
    "            kernel (int): kernel size of the patch embedding\n",
    "            num_heads (int): number of attention heads\n",
    "            attn_reduction (int): attention head spatial reduction factor\n",
    "            ff_expansion (int): expansion factor of the MixFNN\n",
    "            num_layers (int): number of attention layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if kernel != 3 and kernel != 7:\n",
    "            raise ValueError(f'In MiTStage, supported kernel sizes are 3 and 7, but {kernel} is provided')\n",
    "        stride = kernel//2 + 1\n",
    "        padding = stride - 1\n",
    "\n",
    "        self.patching = nn.Unfold(kernel, stride=stride, padding=padding)\n",
    "        self.embedding = nn.Conv2d(c_in * kernel**2, c_out, 1)\n",
    "        self.attention_layers = nn.ModuleList([])\n",
    "        for _ in range(num_layers):\n",
    "            attention = nn.Sequential(LayerNorm(c_out), SpatialReductionAttention(dim=c_out,\n",
    "                                                                                  num_heads=num_heads,\n",
    "                                                                                  reduction=attn_reduction))\n",
    "            ff = nn.Sequential(LayerNorm(c_out), MixFNN(dim=c_out, expansion_factor=ff_expansion))\n",
    "            self.attention_layers.append(nn.ModuleList([attention, ff]))\n",
    "\n",
    "    def forward(self, x):  # (N, C, H, W)\n",
    "        h, w = x.size()[-2:]\n",
    "        x = self.patching(x)  # (N, C*k*k, L)\n",
    "\n",
    "        num_patches = x.size()[-1]\n",
    "        dim_reduction = int(sqrt((h * w) / num_patches))  # this works for arbitrary H and W, rather than only for multiples of 4\n",
    "        x = rearrange(x, 'n c (h w) -> n c h w', h = h // dim_reduction)  # (N, C*k*k, H//S, W//S)\n",
    "        x = self.embedding(x)  # (N, C_out, H//S, W//S)\n",
    "\n",
    "        for (attention, ff) in self.attention_layers:\n",
    "            x = attention(x) + x  # (N, C_out, H//S, W//S)\n",
    "            x = ff(x) + x  # (N, C_out, H//S, W//S)\n",
    "\n",
    "        return x  # (N, C_out, H//S, W//S)\n",
    "\n",
    "\n",
    "class MiT(nn.Module):\n",
    "    def __init__(self, name=None, kernels=None, stage_channels=None, num_heads=None,\n",
    "                 attn_reductions=None, ff_expansions=None, num_layers=None, in_channels=3):\n",
    "        \"\"\"MiT encoder for SegFormer\n",
    "\n",
    "        Args:\n",
    "            name (str, optional): Name of the MiT model. Can be either b0, b2 or b5. Defaults to None.\n",
    "            kernels (tuple, optional): Tuple of MiT stages' kernel sizes. Must be specified if 'name' argument is not provided. Defaults to None.\n",
    "            stage_channels (tuple, optional): Tuple of MiT stages' channel numbers. Must be specified if 'name' argument is not provided. Defaults to None.\n",
    "            num_heads (tuple, optional): Tuple of MiT stages' attention head numbers. Must be specified if 'name' argument is not provided. Defaults to None.\n",
    "            attn_reductions (tuple, optional): Tuple of MiT stages' attention reduction factors. Must be specified if 'name' argument is not provided. Defaults to None.\n",
    "            ff_expansions (tuple, optional): Tuple of MiT stages' feed-forward expansions factors. Must be specified if 'name' argument is not provided. Defaults to None.\n",
    "            num_layers (tuple, optional): Tuple of MiT stages' attention layer numbers. Must be specified if 'name' argument is not provided. Defaults to None.\n",
    "            in_channels (int, optional): Number of channels in the input. Defaults to 3.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        if name is not None:\n",
    "            kernels = (7, 3, 3, 3)\n",
    "            num_heads = (1, 2, 5, 8)\n",
    "            attn_reductions = (8, 4, 2, 1)\n",
    "\n",
    "        if name=='b0':\n",
    "            stage_channels = (32, 64, 160, 256)\n",
    "            ff_expansions = (8, 8, 4, 4)\n",
    "            num_layers = (2, 2, 2, 2)\n",
    "        elif name=='b2':\n",
    "            stage_channels = (64, 128, 320, 512)\n",
    "            ff_expansions = (8, 8, 4, 4)\n",
    "            num_layers = (3, 3, 6, 3)\n",
    "        elif name=='b5':\n",
    "            stage_channels = (64, 128, 320, 512)\n",
    "            ff_expansions = (4, 4, 4, 4)\n",
    "            num_layers = (3, 6, 40, 3)\n",
    "        elif name is not None:\n",
    "            raise ValueError(f'Only b0, b2 and b5 MiT models are supported, but {name} was provided')\n",
    "        \n",
    "        self.stage_channels = stage_channels\n",
    "        c_out = stage_channels\n",
    "        c_in = (in_channels,) + c_out[:-1]\n",
    "\n",
    "        args = (c_in, c_out, kernels, num_heads, attn_reductions, ff_expansions, num_layers)\n",
    "        \n",
    "        if name is None:\n",
    "            if sum([arg is None for arg in args]):\n",
    "                raise ValueError('For customly configured MiT modules, all arguments except name and in_channels must be provided ')\n",
    "            if sum([len(arg)!=len(kernels) for arg in args]):\n",
    "                raise ValueError('All the arguments in MiT except the name and in_channels must be of the same length')\n",
    "\n",
    "        self.stages = nn.ModuleList([MiTStage(*unpacked_args) for unpacked_args in zip(*args)])\n",
    "\n",
    "    def forward(self, x, return_stage_outputs=False):\n",
    "        stage_outputs = []\n",
    "        for stage in self.stages:\n",
    "            x = stage(x) \n",
    "            stage_outputs.append(x)\n",
    "        \n",
    "        return stage_outputs if return_stage_outputs else x\n",
    "    \n",
    "\n",
    "class SegFormer(nn.Module):\n",
    "    def __init__(self, mit=None, in_channels=3, decoder_dim=256, num_classes=4,\n",
    "                 kernels=None, stage_channels=None, num_heads=None,\n",
    "                 attn_reductions=None, ff_expansions=None, num_layers=None):\n",
    "        \"\"\"SegFormer - transformer-based semantic segmentation module. Consists of an MiT encoder\n",
    "        and a segmentation head\n",
    "\n",
    "        Args:\n",
    "            mit (str, optional): Name of the MiT model. Can be either b0, b2 or b5. Defaults to None.\n",
    "            in_channels (int, optional): Number of channels in the input. Defaults to 3.\n",
    "            decoder_dim (int, optional): Number of hidden dimensions in the sigmentation head. Defaults to 256.\n",
    "            num_classes (int, optional): Number of segmentation classes. Defaults to 4.\n",
    "            kernels (tuple, optional): Tuple of MiT stages' kernel sizes. Must be specified if 'mit' argument is not provided. Defaults to None.\n",
    "            stage_channels (tuple, optional): Tuple of MiT stages' channel numbers. Must be specified if 'mit' argument is not provided. Defaults to None.\n",
    "            num_heads (tuple, optional): Tuple of MiT stages' attention head numbers. Must be specified if 'mit' argument is not provided. Defaults to None.\n",
    "            attn_reductions (tuple, optional): Tuple of MiT stages' attention reduction factors. Must be specified if 'mit' argument is not provided. Defaults to None.\n",
    "            ff_expansions (tuple, optional): Tuple of MiT stages' feed-forward expansions factors. Must be specified if 'mit' argument is not provided. Defaults to None.\n",
    "            num_layers (tuple, optional): Tuple of MiT stages' attention layer numbers. Must be specified if 'mit' argument is not provided. Defaults to None.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.mit = MiT(mit, kernels, stage_channels, num_heads,\n",
    "                       attn_reductions, ff_expansions, num_layers, in_channels)\n",
    "        dims = self.mit.stage_channels\n",
    "        self.to_fuse = nn.ModuleList([nn.Sequential(nn.Conv2d(c, decoder_dim, 1),\n",
    "                                                    nn.Upsample(scale_factor = 2**i))\n",
    "                                                    for i, c in enumerate(dims)])\n",
    "        self.segmentation_head = nn.Sequential(nn.Conv2d(4*decoder_dim, decoder_dim, 1),\n",
    "                                               nn.Conv2d(decoder_dim, num_classes, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        stage_outputs = self.mit(x, return_stage_outputs=True)\n",
    "        stage_outputs = [to_fuse(out) for out, to_fuse in zip(stage_outputs, self.to_fuse)]\n",
    "        fused_outputs = torch.cat(stage_outputs, dim=1)\n",
    "        return self.segmentation_head(fused_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 20, 128, 256])\n"
     ]
    }
   ],
   "source": [
    "segformer = SegFormer(mit='b0', num_classes=20)\n",
    "x = torch.rand(16, 3, 512, 1024)\n",
    "y = segformer(x)\n",
    "print(y.size())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
