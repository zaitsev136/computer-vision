{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zaits\\anaconda3\\envs\\cuda\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "import segmentation_models_pytorch as smp \n",
    "\n",
    "from torchvision.datasets import Cityscapes\n",
    "from torchvision import transforms as T\n",
    "import torchvision.models as models\n",
    "\n",
    "from torchmetrics.segmentation import MeanIoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_INDEX = 255\n",
    "void_classes = [0, 1, 2, 3, 4, 5, 6, 9, 10, 14, 15, 16, 18, 29, 30, -1]\n",
    "valid_classes = [IGNORE_INDEX, 7, 8, 11, 12, 13, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 31, 32, 33]\n",
    "class_names = ['unlabelled', 'road', 'sidewalk', 'building', 'wall', 'fence', 'pole', 'traffic_light', \\\n",
    "               'traffic_sign', 'vegetation', 'terrain', 'sky', 'person', 'rider', 'car', 'truck', 'bus', \\\n",
    "               'train', 'motorcycle', 'bicycle']\n",
    "\n",
    "class_map = dict(zip(valid_classes, range(len(valid_classes))))\n",
    "n_classes = len(valid_classes)\n",
    "class_map\n",
    "\n",
    "colors = [\n",
    "    [0, 0, 0],\n",
    "    [128, 64, 128],\n",
    "    [244, 35, 232],\n",
    "    [70, 70, 70],\n",
    "    [102, 102, 156],\n",
    "    [190, 153, 153],\n",
    "    [153, 153, 153],\n",
    "    [250, 170, 30],\n",
    "    [220, 220, 0],\n",
    "    [107, 142, 35],\n",
    "    [152, 251, 152],\n",
    "    [0, 130, 180],\n",
    "    [220, 20, 60],\n",
    "    [255, 0, 0],\n",
    "    [0, 0, 142],\n",
    "    [0, 0, 70],\n",
    "    [0, 60, 100],\n",
    "    [0, 80, 100],\n",
    "    [0, 0, 230],\n",
    "    [119, 11, 32],\n",
    "    ]\n",
    "\n",
    "label_colours = dict(zip(range(n_classes), colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_segmap(mask):\n",
    "    #remove unwanted classes and recitify the labels of wanted classes\n",
    "    for _voidc in void_classes:\n",
    "        mask[mask == _voidc] = IGNORE_INDEX\n",
    "    for _validc in valid_classes:\n",
    "        mask[mask == _validc] = class_map[_validc]\n",
    "    return mask\n",
    "\n",
    "def decode_segmap(mask, return_np=False, color_last=True):\n",
    "    #convert gray scale to color\n",
    "    temp = mask.cpu().numpy()\n",
    "    r = temp.copy()\n",
    "    g = temp.copy()\n",
    "    b = temp.copy()\n",
    "    for l in range(0, n_classes):\n",
    "        r[temp == l] = label_colours[l][0]\n",
    "        g[temp == l] = label_colours[l][1]\n",
    "        b[temp == l] = label_colours[l][2]\n",
    "    \n",
    "    rgb = np.zeros(list(temp.shape)+[3])\n",
    "    rgb[..., 0] = r / 255.0\n",
    "    rgb[..., 1] = g / 255.0\n",
    "    rgb[..., 2] = b / 255.0\n",
    "\n",
    "    if not color_last:\n",
    "        if len(rgb.shape)==4:\n",
    "            rgb = np.transpose(rgb, (0, 3, 1, 2))\n",
    "        elif len(rgb.shape)==3:\n",
    "            rgb = np.transpose(rgb, (2, 0, 1))\n",
    "        else:\n",
    "            raise ValueError(f'mask must have shape either (H,W) or (N,H,W), but {mask.size()} is provided')\n",
    "\n",
    "    if return_np:\n",
    "        return rgb\n",
    "    else:\n",
    "        return torch.from_numpy(rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    A.HorizontalFlip(),\n",
    "    A.ColorJitter(hue=0),\n",
    "    A.Normalize(),\n",
    "    ToTensorV2()])\n",
    "\n",
    "class CityscapesPreprocessedDataset(Dataset):\n",
    "    def __init__(self, root, split, transforms=None):\n",
    "        self.root = root\n",
    "        self.split = split\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = np.load(os.path.join(self.root, self.split, f'{index}.npy'))\n",
    "        y = np.load(os.path.join(self.root, self.split, f'y{index}.npy'))\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            transformed = self.transforms(image=np.array(x), mask=np.array(y))\n",
    "            x = transformed['image']\n",
    "            y = transformed['mask']\n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.split=='train':\n",
    "            return 2975\n",
    "        elif self.split=='val':\n",
    "            return 500\n",
    "        elif self.split=='test':\n",
    "            return 1525\n",
    "        else:\n",
    "            raise ValueError(f'split must be one of {\"train\", \"val\", \"test\"}, but {self.split} is given')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        skip_channels,\n",
    "        out_channels\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels + skip_channels, out_channels, 3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x, skip=None):\n",
    "        x = F.interpolate(x, scale_factor=2)\n",
    "        if skip is not None:\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x, inplace=True)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x, inplace=True)\n",
    "        return x\n",
    "    \n",
    "# UNet model with ResNet backbone\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, out_channels, resnet_layers=34):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load pre-trained ResNet model\n",
    "        if resnet_layers == 18:\n",
    "            resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT, )\n",
    "        elif resnet_layers == 34:\n",
    "            resnet = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
    "        else:\n",
    "            raise ValueError(\"ResNet layers must be 18 or 34.\")\n",
    "        \n",
    "        # Remove the fully connected layers and the average pooling layer from ResNet\n",
    "        encoder = nn.Sequential(\n",
    "            *list(resnet.children())[:-2]  # Exclude the final fully connected layer and avg pool\n",
    "        )\n",
    "        \n",
    "        # Extract feature maps from specific layers of ResNet\n",
    "        self.encoder_blocks = nn.ModuleList([\n",
    "            nn.Sequential(*encoder[:3]), # Conv7x7 - BN - ReLU\n",
    "            nn.Sequential(*encoder[3:5]), # MaxPool - ResNet L1\n",
    "            encoder[5], # ResNet L2\n",
    "            encoder[6], # ResNet L3\n",
    "            encoder[7]]) # ResNet L4\n",
    "        \n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            DecoderBlock(512, 256, 256), # in, skip, out\n",
    "            DecoderBlock(256, 128, 128),\n",
    "            DecoderBlock(128, 64, 64),\n",
    "            DecoderBlock(64, 64, 32),\n",
    "            DecoderBlock(32, 0, 16)])\n",
    "        \n",
    "        self.segmentation_head = nn.Conv2d(16, out_channels, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        encoder_outputs = []\n",
    "        for encoder_block in self.encoder_blocks:\n",
    "            x = encoder_block(x)\n",
    "            encoder_outputs.append(x)\n",
    "\n",
    "        skip_connections = encoder_outputs[-2::-1] + [None,] # remove the last one, reverse the rest, add None\n",
    "\n",
    "        # Decoder\n",
    "        for skip, decoder_block in zip(skip_connections, self.decoder_blocks):\n",
    "            x = decoder_block(x, skip)\n",
    "\n",
    "        # Segmentation head\n",
    "        x = self.segmentation_head(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_smp_unet(out_channels, resnet_layers=34):\n",
    "    return smp.Unet(encoder_name=f\"resnet{resnet_layers}\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "                    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "                    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "                    classes=out_channels,                      # model output channels (number of classes in your dataset)\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        # Pred is of shape (batch_size, num_classes, height, width)\n",
    "        # Target is of shape (batch_size, height, width) and contains class indices (not one-hot encoded)\n",
    "        \n",
    "        num_classes = pred.size(1)\n",
    "        pred = F.softmax(pred, dim=1)  # Apply softmax to get class probabilities\n",
    "        target_one_hot = F.one_hot(target.to(torch.int64), num_classes).permute(0, 3, 1, 2)  # Convert target to one-hot\n",
    "\n",
    "        pred = pred.contiguous()\n",
    "        target_one_hot = target_one_hot.contiguous()\n",
    "\n",
    "        # Flatten\n",
    "        pred_flat = pred.view(pred.size(0), pred.size(1), -1)\n",
    "        target_flat = target_one_hot.view(target_one_hot.size(0), target_one_hot.size(1), -1)\n",
    "\n",
    "        intersection = (pred_flat * target_flat).sum(2)\n",
    "        dice_score = (2. * intersection + self.smooth) / (pred_flat.sum(2) + target_flat.sum(2) + self.smooth)\n",
    "        \n",
    "        return 1 - dice_score.mean()  # Return Dice Loss\n",
    "    \n",
    "dice_loss = DiceLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FormattedMeanIoU(nn.Module):\n",
    "    def __init__(self, num_classes, include_background=True, per_class=False):\n",
    "        super().__init__()\n",
    "        self.metric = MeanIoU(num_classes, include_background, per_class)\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        # takes batched prediction before softmax, and one-hot encoded target\n",
    "        return self.metric(torch.argmax(pred, dim=1), target.to(torch.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import LightningModule, Trainer, LightningDataModule\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.tuner import Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.HorizontalFlip(),\n",
    "    A.ColorJitter(hue=0),\n",
    "    A.Normalize(),\n",
    "    ToTensorV2()])\n",
    "\n",
    "INV_NORMALIZE = T.Normalize(\n",
    "            mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "            std=[1/0.229, 1/0.224, 1/0.255]\n",
    "        )\n",
    "\n",
    "class CityscapesDataModule(LightningDataModule):\n",
    "    def __init__(self, data_dir=\"./data/cityscapes_preprocessed\", batch_size=BATCH_SIZE, transforms=transform):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        if stage == \"fit\":\n",
    "            self.train_dataset = CityscapesPreprocessedDataset(self.data_dir, 'train', self.transforms)\n",
    "            self.val_dataset = CityscapesPreprocessedDataset(self.data_dir, 'val', self.transforms)\n",
    "        if stage == \"test\":\n",
    "            self.test_dataset = CityscapesPreprocessedDataset(self.data_dir, 'test', self.transforms)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cityscapes_data_module = CityscapesDataModule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CityscapesSemanticSegmentation(LightningModule):\n",
    "    def __init__(self, model, learning_rate=1e-3, lr_gamma=0.7):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore='model')\n",
    "        self.example_input_array = torch.Tensor(BATCH_SIZE, 3, 256, 512)\n",
    "        self.model = model\n",
    "        self.lr = learning_rate\n",
    "        self.lr_gamma = lr_gamma\n",
    "        self.metrics = FormattedMeanIoU(n_classes)\n",
    "        self.validation_batch = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def _calculate_loss_and_iou(self, x, y, return_prediction=False):\n",
    "        prediction = self.model(x)\n",
    "        metric = self.metrics(prediction, y)\n",
    "        loss = dice_loss(prediction, y)\n",
    "        if return_prediction:\n",
    "             return loss, metric, prediction\n",
    "        else:\n",
    "            return loss, metric\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        loss, iou = self._calculate_loss_and_iou(x, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log(\"train_iou\", iou, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        loss, iou = self._calculate_loss_and_iou(x, y)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        self.log(\"val_iou\", iou)\n",
    "        if self.validation_batch is None:\n",
    "            self.validation_batch = x\n",
    "            self.decoded_targets = decode_segmap(y).to(self.validation_batch).permute(0, 3, 1, 2)\n",
    "            self.validation_images = torch.concat(list(INV_NORMALIZE(self.validation_batch)), dim=1)\n",
    "            self.validation_truth = torch.concat(list(self.decoded_targets), dim=1)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        prediction = self.forward(self.validation_batch)\n",
    "        decoded_predictions = decode_segmap(torch.argmax(prediction, 1)).to(self.validation_batch).permute(0, 3, 1, 2)\n",
    "        \n",
    "        validation_predictions = torch.concat(list(decoded_predictions), dim=1)\n",
    "        image = torch.concat([self.validation_images, self.validation_truth, validation_predictions], dim=2)\n",
    "        self.logger.experiment.add_image('image', image, self.current_epoch)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, self.lr_gamma)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"learning_rate\": 0.01\n",
      "\"lr_gamma\":      0.96\n"
     ]
    }
   ],
   "source": [
    "DEBUG = False\n",
    "LR = 0.01\n",
    "LR_GAMMA = 0.96\n",
    "RESNET_LAYERS = 18\n",
    "LOAD_LAST_CHECKPOINT = False\n",
    "OVERRIDE_HP = False\n",
    "\n",
    "version_name = f'resnet18_LR{LR}_LRG{LR_GAMMA}'\n",
    "\n",
    "model = UNet(n_classes, resnet_layers=RESNET_LAYERS)\n",
    "lightning_model = CityscapesSemanticSegmentation(model, learning_rate=LR, lr_gamma=LR_GAMMA)\n",
    "if LOAD_LAST_CHECKPOINT:\n",
    "    lightning_model = CityscapesSemanticSegmentation.load_from_checkpoint('./checkpoints/last.ckpt', model=model)\n",
    "if OVERRIDE_HP:\n",
    "    lightning_model.hparams['learning_rate'] = LR\n",
    "    lightning_model.lr = LR \n",
    "    lightning_model.hparams['lr_gamma'] = LR_GAMMA\n",
    "    lightning_model.lr_gamma = LR_GAMMA\n",
    "print(lightning_model.hparams)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=os.path.join('lightning_logs', version_name),  # Where to save the checkpoints\n",
    "    filename='{epoch}-{val_loss:.4f}',  # Filename format\n",
    "    monitor='val_loss',  # Metric to monitor\n",
    "    save_last=True,  # Always save the last checkpoint\n",
    "    enable_version_counter = False\n",
    ")\n",
    "\n",
    "logger = TensorBoardLogger('./', version=version_name, log_graph=True)\n",
    "\n",
    "log_every_n_steps = 50\n",
    "trainer = Trainer(callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=400),\n",
    "                             LearningRateMonitor(logging_interval='epoch'),\n",
    "                             checkpoint_callback],\n",
    "                  fast_dev_run=DEBUG,\n",
    "                #   limit_train_batches=0.1,\n",
    "                #   limit_val_batches=0.1,\n",
    "                #   profiler='simple',\n",
    "                #  max_epochs=30,\n",
    "                  precision='bf16-mixed',\n",
    "                  logger=logger,\n",
    "                #  log_every_n_steps=log_every_n_steps\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuner = Tuner(trainer)\n",
    "# lr_finder = tuner.lr_find(lightning_model, cityscapes_data_module)\n",
    "\n",
    "# # Plot \n",
    "# fig = lr_finder.plot(suggest=True)\n",
    "# fig.show()\n",
    "\n",
    "# lightning_model.hparams.lr = lr_finder.suggestion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zaits\\anaconda3\\envs\\cuda\\Lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4070') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "c:\\Users\\zaits\\anaconda3\\envs\\cuda\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:652: Checkpoint directory C:\\main\\repos\\semantic_segmentation\\lightning_logs\\resnet18_LR0.01_LRG0.96 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type             | Params | Mode  | In sizes          | Out sizes         \n",
      "----------------------------------------------------------------------------------------------\n",
      "0 | model   | UNet             | 14.3 M | train | [32, 3, 256, 512] | [32, 20, 256, 512]\n",
      "1 | metrics | FormattedMeanIoU | 0      | train | ?                 | ?                 \n",
      "----------------------------------------------------------------------------------------------\n",
      "14.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "14.3 M    Total params\n",
      "57.314    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zaits\\anaconda3\\envs\\cuda\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zaits\\anaconda3\\envs\\cuda\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 202:  78%|███████▊  | 73/93 [00:14<00:04,  4.88it/s, v_num=0.96, train_loss=0.327]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zaits\\anaconda3\\envs\\cuda\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(lightning_model, datamodule=cityscapes_data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in cityscapes_data_module.val_dataloader():\n",
    "    prediction = lightning_model(x.to(device=lightning_model.device))\n",
    "    print(prediction.size())\n",
    "    decoded_predictions = decode_segmap(torch.argmax(prediction, 1))\n",
    "    print(decoded_predictions.size())\n",
    "    plt.imshow(decoded_predictions[1].detach().cpu().numpy())\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
