{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "import segmentation_models_pytorch as smp \n",
    "\n",
    "from torchvision.datasets import Cityscapes\n",
    "from torchvision import transforms as T\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_INDEX = 255\n",
    "void_classes = [0, 1, 2, 3, 4, 5, 6, 9, 10, 14, 15, 16, 18, 29, 30, -1]\n",
    "valid_classes = [IGNORE_INDEX, 7, 8, 11, 12, 13, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 31, 32, 33]\n",
    "class_names = ['unlabelled', 'road', 'sidewalk', 'building', 'wall', 'fence', 'pole', 'traffic_light', \\\n",
    "               'traffic_sign', 'vegetation', 'terrain', 'sky', 'person', 'rider', 'car', 'truck', 'bus', \\\n",
    "               'train', 'motorcycle', 'bicycle']\n",
    "\n",
    "class_map = dict(zip(valid_classes, range(len(valid_classes))))\n",
    "n_classes = len(valid_classes)\n",
    "class_map\n",
    "\n",
    "colors = [\n",
    "    [0, 0, 0],\n",
    "    [128, 64, 128],\n",
    "    [244, 35, 232],\n",
    "    [70, 70, 70],\n",
    "    [102, 102, 156],\n",
    "    [190, 153, 153],\n",
    "    [153, 153, 153],\n",
    "    [250, 170, 30],\n",
    "    [220, 220, 0],\n",
    "    [107, 142, 35],\n",
    "    [152, 251, 152],\n",
    "    [0, 130, 180],\n",
    "    [220, 20, 60],\n",
    "    [255, 0, 0],\n",
    "    [0, 0, 142],\n",
    "    [0, 0, 70],\n",
    "    [0, 60, 100],\n",
    "    [0, 80, 100],\n",
    "    [0, 0, 230],\n",
    "    [119, 11, 32],\n",
    "    ]\n",
    "\n",
    "label_colours = dict(zip(range(n_classes), colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_segmap(mask):\n",
    "    #remove unwanted classes and recitify the labels of wanted classes\n",
    "    for _voidc in void_classes:\n",
    "        mask[mask == _voidc] = IGNORE_INDEX\n",
    "    for _validc in valid_classes:\n",
    "        mask[mask == _validc] = class_map[_validc]\n",
    "    return mask\n",
    "\n",
    "def decode_segmap(mask, return_np=False):\n",
    "    #convert gray scale to color\n",
    "    temp = mask.cpu().numpy()\n",
    "    r = temp.copy()\n",
    "    g = temp.copy()\n",
    "    b = temp.copy()\n",
    "    for l in range(0, n_classes):\n",
    "        r[temp == l] = label_colours[l][0]\n",
    "        g[temp == l] = label_colours[l][1]\n",
    "        b[temp == l] = label_colours[l][2]\n",
    "\n",
    "    rgb = np.zeros((temp.shape[0], temp.shape[1], 3))\n",
    "    rgb[:, :, 0] = r / 255.0\n",
    "    rgb[:, :, 1] = g / 255.0\n",
    "    rgb[:, :, 2] = b / 255.0\n",
    "    if return_np:\n",
    "        return rgb\n",
    "    else:\n",
    "        return torch.from_numpy(rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    A.Resize(256, 512),\n",
    "    # A.RandomCrop(256, 256),\n",
    "    A.HorizontalFlip(),\n",
    "    A.ColorJitter(hue=0),\n",
    "    A.Normalize(),\n",
    "    ToTensorV2()])\n",
    "\n",
    "class CityscapesDataset(Cityscapes):\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.images[index]).convert('RGB')\n",
    "        smnt = Image.open(self.targets[index][0])\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            transformed = self.transforms(image=np.array(img), mask=np.array(smnt))\n",
    "            img = transformed['image']\n",
    "            smnt = transformed['mask']\n",
    "        smnt = encode_segmap(smnt)\n",
    "        return img, smnt\n",
    "        \n",
    "# train_dataset = CityscapesDataset('./data/cityscapes', split='train', mode='fine',\n",
    "#                                   target_type='semantic', transforms=transform)\n",
    "\n",
    "# val_dataset = CityscapesDataset('./data/cityscapes', split='val', mode='fine',\n",
    "#                                 target_type='semantic', transforms=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        skip_channels,\n",
    "        out_channels\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels + skip_channels, out_channels, 3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x, skip=None):\n",
    "        x = F.interpolate(x, scale_factor=2)\n",
    "        if skip is not None:\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x, inplace=True)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x, inplace=True)\n",
    "        return x\n",
    "    \n",
    "# UNet model with ResNet backbone\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, out_channels, resnet_layers=34):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load pre-trained ResNet model\n",
    "        if resnet_layers == 18:\n",
    "            resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT, )\n",
    "        elif resnet_layers == 34:\n",
    "            resnet = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
    "        else:\n",
    "            raise ValueError(\"ResNet layers must be 18 or 34.\")\n",
    "        \n",
    "        # Remove the fully connected layers and the average pooling layer from ResNet\n",
    "        encoder = nn.Sequential(\n",
    "            *list(resnet.children())[:-2]  # Exclude the final fully connected layer and avg pool\n",
    "        )\n",
    "        \n",
    "        # Extract feature maps from specific layers of ResNet\n",
    "        self.encoder_blocks = nn.ModuleList([\n",
    "            nn.Sequential(*encoder[:3]), # Conv7x7 - BN - ReLU\n",
    "            nn.Sequential(*encoder[3:5]), # MaxPool - ResNet L1\n",
    "            encoder[5], # ResNet L2\n",
    "            encoder[6], # ResNet L3\n",
    "            encoder[7]]) # ResNet L4\n",
    "        \n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            DecoderBlock(512, 256, 256), # in, skip, out\n",
    "            DecoderBlock(256, 128, 128),\n",
    "            DecoderBlock(128, 64, 64),\n",
    "            DecoderBlock(64, 64, 32),\n",
    "            DecoderBlock(32, 0, 16)])\n",
    "        \n",
    "        self.segmentation_head = nn.Conv2d(16, out_channels, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        encoder_outputs = []\n",
    "        for encoder_block in self.encoder_blocks:\n",
    "            x = encoder_block(x)\n",
    "            encoder_outputs.append(x)\n",
    "\n",
    "        skip_connections = encoder_outputs[-2::-1] + [None,] # remove the last one, reverse the rest, add None\n",
    "\n",
    "        # Decoder\n",
    "        for skip, decoder_block in zip(skip_connections, self.decoder_blocks):\n",
    "            x = decoder_block(x, skip)\n",
    "\n",
    "        # Segmentation head\n",
    "        x = self.segmentation_head(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_smp_unet(out_channels, resnet_layers=34):\n",
    "    return smp.Unet(encoder_name=f\"resnet{resnet_layers}\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "                    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "                    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "                    classes=out_channels,                      # model output channels (number of classes in your dataset)\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        # Pred is of shape (batch_size, num_classes, height, width)\n",
    "        # Target is of shape (batch_size, height, width) and contains class indices (not one-hot encoded)\n",
    "        \n",
    "        num_classes = pred.size(1)\n",
    "        pred = F.softmax(pred, dim=1)  # Apply softmax to get class probabilities\n",
    "        target_one_hot = F.one_hot(target.to(torch.int64), num_classes).permute(0, 3, 1, 2)  # Convert target to one-hot\n",
    "\n",
    "        pred = pred.contiguous()\n",
    "        target_one_hot = target_one_hot.contiguous()\n",
    "\n",
    "        # Flatten\n",
    "        pred_flat = pred.view(pred.size(0), pred.size(1), -1)\n",
    "        target_flat = target_one_hot.view(target_one_hot.size(0), target_one_hot.size(1), -1)\n",
    "\n",
    "        intersection = (pred_flat * target_flat).sum(2)\n",
    "        dice_score = (2. * intersection + self.smooth) / (pred_flat.sum(2) + target_flat.sum(2) + self.smooth)\n",
    "        \n",
    "        return 1 - dice_score.mean()  # Return Dice Loss\n",
    "    \n",
    "dice_loss = DiceLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writer = SummaryWriter('runs/feature_maps_experiment')\n",
    "\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     for img, smnt in train_loader:\n",
    "#         break\n",
    "\n",
    "# # Forward pass to get feature maps\n",
    "# def log_feature_maps(model, inputs, writer, step):\n",
    "#     with torch.no_grad():\n",
    "#         # Forward pass through the model\n",
    "#         x = inputs\n",
    "#         for name, layer in model.named_children():\n",
    "#             x = layer(x)\n",
    "#             print(name)\n",
    "#             # Log feature maps from a specific layer\n",
    "#             #if isinstance(layer, torch.nn.Conv2d) or isinstance(layer, torch.nn.ReLU):\n",
    "#             if True:\n",
    "#                 writer.add_images(f'feature_maps/{name}', x, step)\n",
    "#                 print(name, 'written')\n",
    "#             if 'bottle' in name:\n",
    "#                 break\n",
    "\n",
    "# # Log feature maps\n",
    "# log_feature_maps(model, img[:1].to(device), writer, step=0)\n",
    "\n",
    "# # Close the writer\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import LightningModule, Trainer, LightningDataModule\n",
    "from torchmetrics.segmentation import MeanIoU\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.tuner import Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "INV_NORMALIZE = T.Normalize(\n",
    "            mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "            std=[1/0.229, 1/0.224, 1/0.255]\n",
    "        )\n",
    "\n",
    "class CityscapesDataModule(LightningDataModule):\n",
    "    def __init__(self, data_dir=\"./data/cityscapes\", batch_size=BATCH_SIZE, transforms=transform):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        if stage == \"fit\":\n",
    "            self.train_dataset = CityscapesDataset(self.data_dir, split='train', mode='fine',\n",
    "                                                target_type='semantic', transforms=self.transforms)\n",
    "            self.val_dataset = CityscapesDataset(self.data_dir, split='val', mode='fine',\n",
    "                                                target_type='semantic', transforms=self.transforms)\n",
    "        if stage == \"test\":\n",
    "            self.test_dataset = CityscapesDataset(self.data_dir, split='test', mode='fine',\n",
    "                                                target_type='semantic', transforms=self.transforms)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "    \n",
    "cityscapes_data_module = CityscapesDataModule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CityscapesSemanticSegmentation(LightningModule):\n",
    "    def __init__(self, model, learning_rate=1e-3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore='model')\n",
    "        self.example_input_array = torch.Tensor(BATCH_SIZE, 3, 256, 512)\n",
    "        self.model = model\n",
    "        self.lr = learning_rate\n",
    "        self.metrics = MeanIoU(num_classes=n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def _calculate_loss_and_iou(self, x, y, return_prediction=False):\n",
    "        prediction = self.model(x)\n",
    "        metric = 0  # #self.metrics(prediction, y)\n",
    "        loss = dice_loss(prediction, y)\n",
    "        if return_prediction:\n",
    "             return loss, metric, prediction\n",
    "        else:\n",
    "            return loss, metric\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        loss, iou = self._calculate_loss_and_iou(x, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        self.log(\"train_iou\", iou)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        loss, iou, prediction = self._calculate_loss_and_iou(x, y, return_prediction=True)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        self.log(\"val_iou\", iou)\n",
    "\n",
    "        # log images. Todo: do it once per epoch!\n",
    "        tensorboard_logger = self.logger.experiment\n",
    "        i = 0\n",
    "        invimg = INV_NORMALIZE(x[i])\n",
    "        decoded_target = decode_segmap(y[i]).to(invimg).permute(2, 0, 1)\n",
    "        decoded_prediction = decode_segmap(torch.argmax(prediction[i], 0)).to(invimg).permute(2, 0, 1)\n",
    "        image = torch.concat([invimg, decoded_target], dim=2)\n",
    "        tensorboard_logger.add_image('image', image, self.current_epoch)\n",
    "        tensorboard_logger.add_image('image_output', decoded_prediction, self.current_epoch)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "# checkpoint_callback = ModelCheckpoint(monitor='val_loss',dirpath='checkpoints',\n",
    "#                                         filename='file',save_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "DEBUG = False\n",
    "model = UNet(n_classes, resnet_layers=18)\n",
    "LR = 7e-3\n",
    "lightning_model = CityscapesSemanticSegmentation(model, learning_rate=LR)\n",
    "\n",
    "logger = TensorBoardLogger('./')\n",
    "log_every_n_steps = 5\n",
    "trainer = Trainer(callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3)], fast_dev_run=DEBUG,\n",
    "                  limit_train_batches=0.1, limit_val_batches=0.1,\n",
    "                  profiler='simple',\n",
    "                  max_epochs=2,\n",
    "                  precision='bf16-mixed',\n",
    "                  logger=logger,\n",
    "                  log_every_n_steps=log_every_n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\zaits\\anaconda3\\envs\\cuda\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\zaits\\anaconda3\\envs\\cuda\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "Finding best initial lr: 100%|██████████| 50/50 [03:18<00:00,  2.45s/it]`Trainer.fit` stopped: `max_steps=50` reached.\n",
      "Finding best initial lr: 100%|██████████| 50/50 [03:18<00:00,  3.96s/it]\n",
      "Learning rate set to 0.036307805477010104\n",
      "Restoring states from the checkpoint path at c:\\main\\repos\\semantic_segmentation\\.lr_find_b8f3090e-c457-425b-8baa-e6351ad40fe5.ckpt\n",
      "c:\\Users\\zaits\\anaconda3\\envs\\cuda\\Lib\\site-packages\\lightning_fabric\\utilities\\cloud_io.py:57: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "Restored all states from the checkpoint at c:\\main\\repos\\semantic_segmentation\\.lr_find_b8f3090e-c457-425b-8baa-e6351ad40fe5.ckpt\n",
      "FIT Profiler Report\n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|  Action                                                                                                                                                  \t|  Mean duration (s)\t|  Num calls      \t|  Total time (s) \t|  Percentage %   \t|\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|  Total                                                                                                                                                   \t|  -              \t|  5437           \t|  928.66         \t|  100 %          \t|\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|  [Callback]LearningRateFinder.on_fit_start                                                                                                               \t|  234.68         \t|  3              \t|  704.05         \t|  75.814         \t|\n",
      "|  run_training_epoch                                                                                                                                      \t|  70.346         \t|  10             \t|  703.46         \t|  75.75          \t|\n",
      "|  [_TrainingEpochLoop].train_dataloader_next                                                                                                              \t|  2.2045         \t|  150            \t|  330.67         \t|  35.608         \t|\n",
      "|  [_EvaluationLoop].val_next                                                                                                                              \t|  2.0047         \t|  145            \t|  290.68         \t|  31.301         \t|\n",
      "|  [Strategy]SingleDeviceStrategy.validation_step                                                                                                          \t|  0.20293        \t|  145            \t|  29.425         \t|  3.1686         \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.optimizer_step                                                                                          \t|  0.19433        \t|  150            \t|  29.15          \t|  3.1389         \t|\n",
      "|  run_training_batch                                                                                                                                      \t|  0.19433        \t|  150            \t|  29.15          \t|  3.1389         \t|\n",
      "|  [Strategy]SingleDeviceStrategy.training_step                                                                                                            \t|  0.18459        \t|  150            \t|  27.688         \t|  2.9815         \t|\n",
      "|  [Callback]_LRCallback.on_train_batch_end                                                                                                                \t|  0.14379        \t|  150            \t|  21.568         \t|  2.3225         \t|\n",
      "|  [Strategy]SingleDeviceStrategy.batch_to_device                                                                                                          \t|  0.0050814      \t|  295            \t|  1.499          \t|  0.16142        \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.transfer_batch_to_device                                                                                \t|  0.0049254      \t|  295            \t|  1.453          \t|  0.15646        \t|\n",
      "|  [Strategy]SingleDeviceStrategy.backward                                                                                                                 \t|  0.00768        \t|  150            \t|  1.152          \t|  0.12405        \t|\n",
      "|  [LightningDataModule]CityscapesDataModule.setup                                                                                                         \t|  0.021          \t|  3              \t|  0.063          \t|  0.006784       \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.optimizer_zero_grad                                                                                     \t|  0.00010667     \t|  150            \t|  0.016          \t|  0.0017229      \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.on_validation_model_zero_grad                                                                           \t|  0.0015         \t|  10             \t|  0.015          \t|  0.0016152      \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.configure_callbacks                                                                                     \t|  0.0            \t|  3              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]LearningRateFinder.setup                                                                                                                      \t|  0.0            \t|  3              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_loss', 'mode': 'min'}.setup                                                                                     \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]TQDMProgressBar.setup                                                                                                                         \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelSummary.setup                                                                                                                            \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.setup             \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.setup                                                                                                   \t|  0.0            \t|  3              \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.configure_optimizers                                                                                    \t|  0.0            \t|  3              \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningDataModule]CityscapesDataModule.state_dict                                                                                                    \t|  0.0            \t|  3              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]LearningRateFinder.on_save_checkpoint                                                                                                         \t|  0.0            \t|  3              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_loss', 'mode': 'min'}.on_save_checkpoint                                                                        \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]TQDMProgressBar.on_save_checkpoint                                                                                                            \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelSummary.on_save_checkpoint                                                                                                               \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_save_checkpoint\t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.on_save_checkpoint                                                                                      \t|  0.0            \t|  3              \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningDataModule]CityscapesDataModule.train_dataloader                                                                                              \t|  0.0            \t|  3              \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningDataModule]CityscapesDataModule.val_dataloader                                                                                                \t|  0.0            \t|  3              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]_LRCallback.on_train_start                                                                                                                    \t|  0.0            \t|  3              \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.on_train_start                                                                                          \t|  0.0            \t|  3              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Strategy]SingleDeviceStrategy.on_train_start                                                                                                           \t|  0.0            \t|  3              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]_LRCallback.on_train_epoch_start                                                                                                              \t|  0.0            \t|  10             \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.on_train_epoch_start                                                                                    \t|  0.0            \t|  10             \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.on_validation_model_eval                                                                                \t|  0.0            \t|  10             \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]_LRCallback.on_validation_start                                                                                                               \t|  0.0            \t|  10             \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.on_validation_start                                                                                     \t|  0.0            \t|  10             \t|  0.0            \t|  0.0            \t|\n",
      "|  [Strategy]SingleDeviceStrategy.on_validation_start                                                                                                      \t|  0.0            \t|  10             \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]_LRCallback.on_validation_epoch_start                                                                                                         \t|  0.0            \t|  10             \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.on_validation_epoch_start                                                                               \t|  0.0            \t|  10             \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.on_before_batch_transfer                                                                                \t|  0.0            \t|  295            \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.on_after_batch_transfer                                                                                 \t|  0.0            \t|  295            \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]_LRCallback.on_validation_batch_start                                                                                                         \t|  0.0            \t|  145            \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.on_validation_batch_start                                                                               \t|  0.0            \t|  145            \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]_LRCallback.on_exception                                                                                                                      \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningDataModule]CityscapesDataModule.on_exception                                                                                                  \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]_LRCallback.setup                                                                                                                             \t|  0.0            \t|  2              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]_LRCallback.on_save_checkpoint                                                                                                                \t|  0.0            \t|  2              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]_LRCallback.on_validation_batch_end                                                                                                           \t|  0.0            \t|  144            \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.on_validation_batch_end                                                                                 \t|  0.0            \t|  144            \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]_LRCallback.on_validation_epoch_end                                                                                                           \t|  0.0            \t|  9              \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.on_validation_epoch_end                                                                                 \t|  0.0            \t|  9              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]_LRCallback.on_validation_end                                                                                                                 \t|  0.0            \t|  9              \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.on_validation_end                                                                                       \t|  0.0            \t|  9              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Strategy]SingleDeviceStrategy.on_validation_end                                                                                                        \t|  0.0            \t|  9              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]_LRCallback.on_train_batch_start                                                                                                              \t|  0.0            \t|  150            \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.on_train_batch_start                                                                                    \t|  0.0            \t|  150            \t|  0.0            \t|  0.0            \t|\n",
      "|  [Strategy]SingleDeviceStrategy.on_train_batch_start                                                                                                     \t|  0.0            \t|  150            \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]_LRCallback.on_before_zero_grad                                                                                                               \t|  0.0            \t|  150            \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.on_before_zero_grad                                                                                     \t|  0.0            \t|  150            \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]_LRCallback.on_before_backward                                                                                                                \t|  0.0            \t|  150            \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.on_before_backward                                                                                      \t|  0.0            \t|  150            \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]_LRCallback.on_after_backward                                                                                                                 \t|  0.0            \t|  150            \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.on_after_backward                                                                                       \t|  0.0            \t|  150            \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]_LRCallback.on_before_optimizer_step                                                                                                          \t|  0.0            \t|  150            \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.on_before_optimizer_step                                                                                \t|  0.0            \t|  150            \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.configure_gradient_clipping                                                                             \t|  0.0            \t|  150            \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.lr_scheduler_step                                                                                       \t|  0.0            \t|  150            \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.on_train_batch_end                                                                                      \t|  0.0            \t|  150            \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]_LRCallback.on_train_epoch_end                                                                                                                \t|  0.0            \t|  9              \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.on_train_epoch_end                                                                                      \t|  0.0            \t|  9              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]_LRCallback.on_train_end                                                                                                                      \t|  0.0            \t|  2              \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.on_train_end                                                                                            \t|  0.0            \t|  2              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Strategy]SingleDeviceStrategy.on_train_end                                                                                                             \t|  0.0            \t|  2              \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.on_load_checkpoint                                                                                      \t|  0.0            \t|  2              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]LearningRateFinder.on_load_checkpoint                                                                                                         \t|  0.0            \t|  2              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]_LRCallback.on_load_checkpoint                                                                                                                \t|  0.0            \t|  2              \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningDataModule]CityscapesDataModule.teardown                                                                                                      \t|  0.0            \t|  2              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]LearningRateFinder.teardown                                                                                                                   \t|  0.0            \t|  2              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]_LRCallback.teardown                                                                                                                          \t|  0.0            \t|  2              \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.teardown                                                                                                \t|  0.0            \t|  2              \t|  0.0            \t|  0.0            \t|\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tuner = Tuner(trainer)\n",
    "lr_finder = tuner.lr_find(lightning_model, cityscapes_data_module, num_training=50)\n",
    "\n",
    "# Plot \n",
    "fig = lr_finder.plot(suggest=True)\n",
    "fig.show()\n",
    "\n",
    "lightning_model.hparams.lr = lr_finder.suggestion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type    | Params | Mode  | In sizes          | Out sizes         \n",
      "-------------------------------------------------------------------------------------\n",
      "0 | model   | UNet    | 14.3 M | train | [32, 3, 256, 512] | [32, 20, 256, 512]\n",
      "1 | metrics | MeanIoU | 0      | train | ?                 | ?                 \n",
      "-------------------------------------------------------------------------------------\n",
      "14.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "14.3 M    Total params\n",
      "57.314    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zaits\\anaconda3\\envs\\cuda\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zaits\\anaconda3\\envs\\cuda\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 9/9 [00:24<00:00,  0.37it/s, v_num=13, train_loss=0.902]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 9/9 [00:24<00:00,  0.36it/s, v_num=13, train_loss=0.902]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FIT Profiler Report\n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|  Action                                                                                                                                                         \t|  Mean duration (s)\t|  Num calls      \t|  Total time (s) \t|  Percentage %   \t|\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|  Total                                                                                                                                                          \t|  -              \t|  958            \t|  66.375         \t|  100 %          \t|\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|  run_training_epoch                                                                                                                                             \t|  24.43          \t|  2              \t|  48.86          \t|  73.612         \t|\n",
      "|  [_TrainingEpochLoop].train_dataloader_next                                                                                                                     \t|  2.1136         \t|  18             \t|  38.045         \t|  57.318         \t|\n",
      "|  [_EvaluationLoop].val_next                                                                                                                                     \t|  2.0883         \t|  3              \t|  6.265          \t|  9.4388         \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.optimizer_step                                                                                                 \t|  0.18917        \t|  18             \t|  3.405          \t|  5.1299         \t|\n",
      "|  run_training_batch                                                                                                                                             \t|  0.18917        \t|  18             \t|  3.405          \t|  5.1299         \t|\n",
      "|  [Strategy]SingleDeviceStrategy.training_step                                                                                                                   \t|  0.17972        \t|  18             \t|  3.235          \t|  4.8738         \t|\n",
      "|  [Callback]TQDMProgressBar.on_train_batch_end                                                                                                                   \t|  0.13211        \t|  18             \t|  2.378          \t|  3.5827         \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_epoch_end       \t|  0.601          \t|  2              \t|  1.202          \t|  1.8109         \t|\n",
      "|  [Strategy]SingleDeviceStrategy.validation_step                                                                                                                 \t|  0.23967        \t|  3              \t|  0.719          \t|  1.0832         \t|\n",
      "|  [Strategy]SingleDeviceStrategy.backward                                                                                                                        \t|  0.0094444      \t|  18             \t|  0.17           \t|  0.25612        \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.transfer_batch_to_device                                                                                       \t|  0.0055909      \t|  22             \t|  0.123          \t|  0.18531        \t|\n",
      "|  [Strategy]SingleDeviceStrategy.batch_to_device                                                                                                                 \t|  0.0051429      \t|  21             \t|  0.108          \t|  0.16271        \t|\n",
      "|  [Callback]ModelSummary.on_fit_start                                                                                                                            \t|  0.031          \t|  1              \t|  0.031          \t|  0.046704       \t|\n",
      "|  [Callback]TQDMProgressBar.on_validation_start                                                                                                                  \t|  0.0053333      \t|  3              \t|  0.016          \t|  0.024105       \t|\n",
      "|  [Callback]TQDMProgressBar.on_validation_batch_start                                                                                                            \t|  0.0053333      \t|  3              \t|  0.016          \t|  0.024105       \t|\n",
      "|  [Callback]TQDMProgressBar.on_validation_batch_end                                                                                                              \t|  0.0053333      \t|  3              \t|  0.016          \t|  0.024105       \t|\n",
      "|  [Callback]TQDMProgressBar.on_validation_end                                                                                                                    \t|  0.0053333      \t|  3              \t|  0.016          \t|  0.024105       \t|\n",
      "|  [LightningDataModule]CityscapesDataModule.setup                                                                                                                \t|  0.015          \t|  1              \t|  0.015          \t|  0.022599       \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.configure_callbacks                                                                                            \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_loss', 'mode': 'min'}.setup                                                                                            \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]TQDMProgressBar.setup                                                                                                                                \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelSummary.setup                                                                                                                                   \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.setup                    \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.setup                                                                                                          \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.configure_optimizers                                                                                           \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_loss', 'mode': 'min'}.on_fit_start                                                                                     \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]TQDMProgressBar.on_fit_start                                                                                                                         \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.on_before_batch_transfer                                                                                       \t|  0.0            \t|  22             \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.on_after_batch_transfer                                                                                        \t|  0.0            \t|  22             \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_fit_start             \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.on_fit_start                                                                                                   \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_loss', 'mode': 'min'}.on_sanity_check_start                                                                            \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]TQDMProgressBar.on_sanity_check_start                                                                                                                \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelSummary.on_sanity_check_start                                                                                                                   \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_sanity_check_start    \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningDataModule]CityscapesDataModule.val_dataloader                                                                                                       \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.on_validation_model_eval                                                                                       \t|  0.0            \t|  3              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_loss', 'mode': 'min'}.on_validation_start                                                                              \t|  0.0            \t|  3              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelSummary.on_validation_start                                                                                                                     \t|  0.0            \t|  3              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_start      \t|  0.0            \t|  3              \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.on_validation_start                                                                                            \t|  0.0            \t|  3              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Strategy]SingleDeviceStrategy.on_validation_start                                                                                                             \t|  0.0            \t|  3              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_loss', 'mode': 'min'}.on_validation_epoch_start                                                                        \t|  0.0            \t|  3              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]TQDMProgressBar.on_validation_epoch_start                                                                                                            \t|  0.0            \t|  3              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelSummary.on_validation_epoch_start                                                                                                               \t|  0.0            \t|  3              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_epoch_start\t|  0.0            \t|  3              \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.on_validation_epoch_start                                                                                      \t|  0.0            \t|  3              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_loss', 'mode': 'min'}.on_validation_batch_start                                                                        \t|  0.0            \t|  3              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelSummary.on_validation_batch_start                                                                                                               \t|  0.0            \t|  3              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_batch_start\t|  0.0            \t|  3              \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.on_validation_batch_start                                                                                      \t|  0.0            \t|  3              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_loss', 'mode': 'min'}.on_validation_batch_end                                                                          \t|  0.0            \t|  3              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelSummary.on_validation_batch_end                                                                                                                 \t|  0.0            \t|  3              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_batch_end  \t|  0.0            \t|  3              \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.on_validation_batch_end                                                                                        \t|  0.0            \t|  3              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_loss', 'mode': 'min'}.on_validation_epoch_end                                                                          \t|  0.0            \t|  3              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]TQDMProgressBar.on_validation_epoch_end                                                                                                              \t|  0.0            \t|  3              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelSummary.on_validation_epoch_end                                                                                                                 \t|  0.0            \t|  3              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_epoch_end  \t|  0.0            \t|  3              \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.on_validation_epoch_end                                                                                        \t|  0.0            \t|  3              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_loss', 'mode': 'min'}.on_validation_end                                                                                \t|  0.0            \t|  3              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelSummary.on_validation_end                                                                                                                       \t|  0.0            \t|  3              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_end        \t|  0.0            \t|  3              \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.on_validation_end                                                                                              \t|  0.0            \t|  3              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Strategy]SingleDeviceStrategy.on_validation_end                                                                                                               \t|  0.0            \t|  3              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_loss', 'mode': 'min'}.on_sanity_check_end                                                                              \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]TQDMProgressBar.on_sanity_check_end                                                                                                                  \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelSummary.on_sanity_check_end                                                                                                                     \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_sanity_check_end      \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningDataModule]CityscapesDataModule.train_dataloader                                                                                                     \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_loss', 'mode': 'min'}.on_train_start                                                                                   \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]TQDMProgressBar.on_train_start                                                                                                                       \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelSummary.on_train_start                                                                                                                          \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_start           \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.on_train_start                                                                                                 \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Strategy]SingleDeviceStrategy.on_train_start                                                                                                                  \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_loss', 'mode': 'min'}.on_train_epoch_start                                                                             \t|  0.0            \t|  2              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]TQDMProgressBar.on_train_epoch_start                                                                                                                 \t|  0.0            \t|  2              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelSummary.on_train_epoch_start                                                                                                                    \t|  0.0            \t|  2              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_epoch_start     \t|  0.0            \t|  2              \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.on_train_epoch_start                                                                                           \t|  0.0            \t|  2              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_loss', 'mode': 'min'}.on_train_batch_start                                                                             \t|  0.0            \t|  18             \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]TQDMProgressBar.on_train_batch_start                                                                                                                 \t|  0.0            \t|  18             \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelSummary.on_train_batch_start                                                                                                                    \t|  0.0            \t|  18             \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_batch_start     \t|  0.0            \t|  18             \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.on_train_batch_start                                                                                           \t|  0.0            \t|  18             \t|  0.0            \t|  0.0            \t|\n",
      "|  [Strategy]SingleDeviceStrategy.on_train_batch_start                                                                                                            \t|  0.0            \t|  18             \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_loss', 'mode': 'min'}.on_before_zero_grad                                                                              \t|  0.0            \t|  18             \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]TQDMProgressBar.on_before_zero_grad                                                                                                                  \t|  0.0            \t|  18             \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelSummary.on_before_zero_grad                                                                                                                     \t|  0.0            \t|  18             \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_before_zero_grad      \t|  0.0            \t|  18             \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.on_before_zero_grad                                                                                            \t|  0.0            \t|  18             \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.optimizer_zero_grad                                                                                            \t|  0.0            \t|  18             \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_loss', 'mode': 'min'}.on_before_backward                                                                               \t|  0.0            \t|  18             \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]TQDMProgressBar.on_before_backward                                                                                                                   \t|  0.0            \t|  18             \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelSummary.on_before_backward                                                                                                                      \t|  0.0            \t|  18             \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_before_backward       \t|  0.0            \t|  18             \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.on_before_backward                                                                                             \t|  0.0            \t|  18             \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_loss', 'mode': 'min'}.on_after_backward                                                                                \t|  0.0            \t|  18             \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]TQDMProgressBar.on_after_backward                                                                                                                    \t|  0.0            \t|  18             \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelSummary.on_after_backward                                                                                                                       \t|  0.0            \t|  18             \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_after_backward        \t|  0.0            \t|  18             \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.on_after_backward                                                                                              \t|  0.0            \t|  18             \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_loss', 'mode': 'min'}.on_before_optimizer_step                                                                         \t|  0.0            \t|  18             \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]TQDMProgressBar.on_before_optimizer_step                                                                                                             \t|  0.0            \t|  18             \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelSummary.on_before_optimizer_step                                                                                                                \t|  0.0            \t|  18             \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_before_optimizer_step \t|  0.0            \t|  18             \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.on_before_optimizer_step                                                                                       \t|  0.0            \t|  18             \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.configure_gradient_clipping                                                                                    \t|  0.0            \t|  18             \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_loss', 'mode': 'min'}.on_train_batch_end                                                                               \t|  0.0            \t|  18             \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelSummary.on_train_batch_end                                                                                                                      \t|  0.0            \t|  18             \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_batch_end       \t|  0.0            \t|  18             \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.on_train_batch_end                                                                                             \t|  0.0            \t|  18             \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.on_validation_model_zero_grad                                                                                  \t|  0.0            \t|  2              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]TQDMProgressBar.on_train_epoch_end                                                                                                                   \t|  0.0            \t|  2              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelSummary.on_train_epoch_end                                                                                                                      \t|  0.0            \t|  2              \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.on_train_epoch_end                                                                                             \t|  0.0            \t|  2              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_loss', 'mode': 'min'}.on_train_epoch_end                                                                               \t|  0.0            \t|  2              \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningDataModule]CityscapesDataModule.state_dict                                                                                                           \t|  0.0            \t|  2              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_loss', 'mode': 'min'}.on_save_checkpoint                                                                               \t|  0.0            \t|  2              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]TQDMProgressBar.on_save_checkpoint                                                                                                                   \t|  0.0            \t|  2              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelSummary.on_save_checkpoint                                                                                                                      \t|  0.0            \t|  2              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_save_checkpoint       \t|  0.0            \t|  2              \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.on_save_checkpoint                                                                                             \t|  0.0            \t|  2              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_loss', 'mode': 'min'}.on_train_end                                                                                     \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]TQDMProgressBar.on_train_end                                                                                                                         \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelSummary.on_train_end                                                                                                                            \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_end             \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.on_train_end                                                                                                   \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Strategy]SingleDeviceStrategy.on_train_end                                                                                                                    \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_loss', 'mode': 'min'}.on_fit_end                                                                                       \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]TQDMProgressBar.on_fit_end                                                                                                                           \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelSummary.on_fit_end                                                                                                                              \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_fit_end               \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.on_fit_end                                                                                                     \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningDataModule]CityscapesDataModule.teardown                                                                                                             \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_loss', 'mode': 'min'}.teardown                                                                                         \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]TQDMProgressBar.teardown                                                                                                                             \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelSummary.teardown                                                                                                                                \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.teardown                 \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "|  [LightningModule]CityscapesSemanticSegmentation.teardown                                                                                                       \t|  0.0            \t|  1              \t|  0.0            \t|  0.0            \t|\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(lightning_model, datamodule=cityscapes_data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
